{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e0c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import unicodedata\n",
    "\n",
    "from jaxtyping import Int, Float\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55346da0",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91ce564",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int\n",
    "    n_context: int\n",
    "    n_layers: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5d489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Embedding(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "    \n",
    "#     def forward(self):\n",
    "#         pass\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        # self.W_qk = nn.Linear(config.d_model, config.d_vocab)\n",
    "        self.bilinear = nn.Bilinear(config.d_model, config.d_model, config.n_context, bias=False)\n",
    "        self.M = torch.triu(torch.ones((config.n_context, config.n_context)), diagonal=1)\n",
    "        self.M = self.M.masked_fill(self.M.bool(), -torch.inf)\n",
    "        self.second_matmult = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xwx = self.bilinear(x, x) # d_m x d_m\n",
    "        x_masked = xwx+ self.M \n",
    "        x_softmaxed = self.softmax(x_masked)\n",
    "        x_fin = x_softmaxed@x\n",
    "        #multiply softmaxed by x\n",
    "        #multiply that by wov\n",
    "        x_fin = self.second_matmult(x_fin)\n",
    "        return x_fin\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear_up = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear_down = nn.Linear(config.d_hidden, config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_up(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_down(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.MLP = MLP(config=self.config)\n",
    "        self.Attention = Attention(config=self.config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.Attention(x) + self.MLP(x)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config:Config):\n",
    "        self.transformerBlock = nn.ModuleList([TransformerBlock(config) for i in range(config.n_layers)])\n",
    "        self.embedding = nn.Embedding(num_embeddings=config.d_vocab, embedding_dim=config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for i, l in enumerate(self.transformerBlock):\n",
    "            x = self.transformerBlock[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7ee2d",
   "metadata": {},
   "source": [
    "$n_c$: Context window length\n",
    "\n",
    "$d_m$: Model Dimension\n",
    "\n",
    "$d_v$: Vocab Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937b0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"The quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19deb5",
   "metadata": {},
   "source": [
    "# Tokenization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549c941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_gutenberg_book(\n",
    "\tid: int | None = 84,\n",
    "\tdata_temp: Path | str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp: Path = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse: requests.Response = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path | str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1b9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(\n",
    "\ttext: str,\n",
    "\tallowed_punctuation: str = \"-.,;:!?()\\\"\\\\\" + \"\".join(str(x) for x in range(10)),\n",
    "\tpunctuation_convert: dict[str, str] = {'â€”': '-'},\n",
    ") -> str:\n",
    "\t\n",
    "\t# replace some special characters which unicode won't normalize properly\n",
    "\tfor char, replacement in punctuation_convert.items():\n",
    "\t\ttext = text.replace(char, replacement)\n",
    "\n",
    "\t# if a line has \".jpg\" in it, remove that line (this is specific to Don Quixote)\n",
    "\ttext = '\\n'.join(\n",
    "\t\tline \n",
    "\t\tfor line in text.split('\\n')\n",
    "\t\tif '.jpg' not in line\n",
    "\t)\n",
    "\n",
    "\t# Normalize the string to decompose Unicode characters\n",
    "\ttext = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\t# Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "\ttext = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\t# remove newlines and tabs\n",
    "\ttext = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "\t# put spaces around allowed punctuation\n",
    "\tfor char in allowed_punctuation:\n",
    "\t\ttext = text.replace(char, f' {char} ')\n",
    "\n",
    "\n",
    "\t# remove leading and trailing spaces\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# remove multiple spaces\n",
    "\twhile '  ' in text:\n",
    "\t\ttext = text.replace('  ', ' ')\n",
    "\n",
    "\n",
    "\t# remove all characters except (alphanumeric, allowed_punctuation, ' ')\n",
    "\ttext = ''.join(\n",
    "\t\t(\n",
    "\t\t\tchar \n",
    "\t\t\tif (\n",
    "\t\t\t\tchar.isalnum() \n",
    "\t\t\t\tor char in allowed_punctuation \n",
    "\t\t\t\tor char == ' '\n",
    "\t\t\t)\n",
    "\t\t\telse ' '\n",
    "\t\t)\n",
    "\t\tfor char in text \n",
    "\t)\n",
    "\n",
    "\t# convert to lowercase\n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = text.strip()\n",
    "\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad6713ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "\ttext: str,\n",
    "\tprocess: bool = False,\n",
    ") -> list[str]:\n",
    "\tif process:\n",
    "\t\ttext = process_text(text)\n",
    "\treturn text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c274f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 6762...\n",
      "\t584611 characters read\n",
      "Getting book 1497...\n",
      "\t1219052 characters read\n",
      "Getting book 8438...\n",
      "\t648768 characters read\n",
      "Getting book 1600...\n",
      "\t181248 characters read\n",
      "Getting book 1656...\n",
      "\t87284 characters read\n"
     ]
    }
   ],
   "source": [
    "# Getting books from Plato and Aristotle\n",
    "DATA_RAW: list[str] = get_many_books([6762, 1497, 8438, 1600, 1656])\n",
    "DATA: str = \" \".join(process_text(x) for x in DATA_RAW)\n",
    "DATA_TOKENIZED: list[str] = tokenize(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317fb7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ENCODED = array([1181,   25, 9326, ..., 4819, 4354, 1842], shape=(626081,))\n",
      "626081\n"
     ]
    }
   ],
   "source": [
    "# sorted by frequency\n",
    "VOCAB_FREQ: Counter[str] = Counter(DATA_TOKENIZED)\n",
    "VOCAB_ARR: list[str] = [word for word, _ in VOCAB_FREQ.most_common()]\n",
    "VOCAB_DICT: dict[str, int] = {word: i for i, word in enumerate(VOCAB_ARR)}\n",
    "\n",
    "def encode(\n",
    "\ttext: str | list[str],\n",
    ") -> Int[np.ndarray, \" n_tokens\"]:\n",
    "\tif isinstance(text, str):\n",
    "\t\ttext = tokenize(text)\n",
    "\treturn np.array([VOCAB_DICT[word] for word in text])\n",
    "\n",
    "def decode(\n",
    "\tencoded_text: Int[np.ndarray, \" n_tokens\"] | list[int],\n",
    ") -> str:\n",
    "\treturn ' '.join(VOCAB_ARR[i] for i in encoded_text)\n",
    "\n",
    "DATA_ENCODED: Int[np.ndarray, \" n_tokens\"] = encode(DATA)\n",
    "\n",
    "print(f\"{DATA_ENCODED = }\")\n",
    "print(len(DATA_ENCODED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38824d3",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7248308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "tensor([[-1.5844e-01, -1.2614e-01, -1.5132e-01, -7.4821e-02, -3.3354e-02,\n",
      "         -1.7954e-01,  5.4231e-03,  5.0504e-01,  1.9708e-01,  5.3856e-01],\n",
      "        [-2.9039e-02, -4.2993e-01,  3.6675e-02, -8.0885e-02,  1.5491e-01,\n",
      "          3.9740e-01,  1.4762e-01,  2.9777e-01,  4.1032e-01, -1.1207e-01],\n",
      "        [-1.8447e-01, -5.5580e-02,  1.8991e-02,  2.4289e-01,  2.5611e-01,\n",
      "          2.5013e-04, -2.4621e-01,  3.9945e-01,  1.0713e-02,  3.0392e-01],\n",
      "        [-3.0789e-01, -1.8269e-01, -1.0287e-01,  2.2861e-01,  2.4110e-01,\n",
      "         -1.2972e-01, -1.8224e-01,  4.0773e-01,  1.4991e-01,  3.6655e-01],\n",
      "        [ 2.3969e-01,  3.0128e-01,  1.3705e-01,  2.0809e-01, -2.6409e-01,\n",
      "          4.6971e-01, -2.3857e-01,  2.3207e-01, -3.3052e-01,  5.8728e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sawyer\\Documents\\decodingGPT\\transformer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "d_model = 10\n",
    "d_vocab = 10\n",
    "d_hidden = 10\n",
    "n_context = 5\n",
    "n_layers = 10\n",
    "\n",
    "x = torch.randn((n_context, d_model))\n",
    "\n",
    "conf = Config(d_model, d_vocab, d_hidden, n_context, n_layers)\n",
    "mlp = MLP(conf)\n",
    "attention = Attention(conf)\n",
    "Aoutput = attention(x)\n",
    "print(Aoutput.shape)\n",
    "\n",
    "output = mlp(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6758f832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2841,  1.2585, -0.3046,  1.2458,  1.5591, -1.8320, -2.0312,  0.7599,\n",
       "         -0.4450,  0.2922],\n",
       "        [-1.0739,  2.1877, -0.6875,  1.9496, -0.1120,  1.7716, -3.3345, -1.2047,\n",
       "         -0.1815, -0.4620],\n",
       "        [ 1.3334,  1.3246,  0.6766, -0.3026, -0.3247,  0.9369, -1.8377, -0.6661,\n",
       "         -1.1505, -0.4307],\n",
       "        [ 1.0869,  0.6554,  0.4093, -0.9846,  1.3897,  0.2202, -0.2722, -0.9686,\n",
       "          0.1981,  1.2969],\n",
       "        [ 0.1931, -0.3004, -1.2248, -0.1256,  1.0818,  0.3864,  0.1961, -1.7655,\n",
       "         -0.2540, -0.4553]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer Block test\n",
    "\n",
    "d_model = 10\n",
    "d_vocab = len(VOCAB_DICT)\n",
    "d_hidden = 10\n",
    "n_context = 5\n",
    "n_layers = 10\n",
    "\n",
    "config = Config(\n",
    "    d_model = d_model,\n",
    "    d_vocab = d_vocab,\n",
    "    d_hidden = d_hidden,\n",
    "    n_context = n_context,\n",
    "    n_layers = n_layers,\n",
    ")\n",
    "\n",
    "x = torch.randn((n_context, d_model))\n",
    "conf = Config(d_model, d_vocab, d_hidden, n_context, n_layers)\n",
    "\n",
    "tb = TransformerBlock(config)\n",
    "\n",
    "output_x = tb(x)\n",
    "output_x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222519f",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Config(d_model = 10, \n",
    "              d_vocab = len(VOCAB_DICT), \n",
    "              d_hidden = 10, \n",
    "              n_context = 10, \n",
    "              n_layers = 2\n",
    "              )\n",
    "\n",
    "\n",
    "transformer = Transformer(config=conf)\n",
    "output = transformer(DATA_ENCODED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
