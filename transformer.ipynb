{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ce564",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int\n",
    "    n_context: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66313a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.triu(torch.ones((5, 5)), diagonal=1)\n",
    "M = M.masked_fill(M.bool(), -torch.inf)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Embedding(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "    \n",
    "#     def forward(self):\n",
    "#         pass\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        # self.W_qk = nn.Linear(config.d_model, config.d_vocab)\n",
    "        self.bilinear = nn.Bilinear(config.d_model, config.d_model, config.n_context, bias=False)\n",
    "        self.M = torch.triu(torch.ones((config.n_context, config.n_context)), diagonal=1)\n",
    "        self.M = M.masked_fill(M.bool(), -torch.inf)\n",
    "        self.second_matmult = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xwx = self.bilinear(x) # d_m x d_m\n",
    "        x_masked = xwx+ self.M \n",
    "        x_softmaxed = self.softmax(x_masked)\n",
    "        x_fin = x_softmaxed@x\n",
    "        #multiply softmaxed by x\n",
    "        #multiply that by wov\n",
    "        x_fin = self.second_matmult(x_fin)\n",
    "        return x_fin\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear_up = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear_down = nn.Linear(config.d_hidden, config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_up(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_down(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.MLP = MLP(config=self.config)\n",
    "        self.Attention = Attention(config=self.config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.Attention(x) + self.MLP(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7ee2d",
   "metadata": {},
   "source": [
    "$n_c$: Context window length\n",
    "\n",
    "$d_m$: Model Dimension\n",
    "\n",
    "$d_v$: Vocab Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"The quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ed903",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 10\n",
    "d_vocab = 10\n",
    "d_hidden = 10\n",
    "n_context = 10\n",
    "\n",
    "conf = Config(d_model, d_vocab, d_hidden, n_context)\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=conf.d_vocab, embedding_dim=conf.d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_gutenberg_book(\n",
    "\tid: int | None = 84,\n",
    "\tdata_temp: Path | str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp: Path = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse: requests.Response = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path | str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(\n",
    "\ttext: str,\n",
    "\tallowed_punctuation: str = \"-.,;:!?()\\\"\\\\\" + \"\".join(str(x) for x in range(10)),\n",
    "\tpunctuation_convert: dict[str, str] = {'â€”': '-'},\n",
    ") -> str:\n",
    "\t\n",
    "\t# replace some special characters which unicode won't normalize properly\n",
    "\tfor char, replacement in punctuation_convert.items():\n",
    "\t\ttext = text.replace(char, replacement)\n",
    "\n",
    "\t# if a line has \".jpg\" in it, remove that line (this is specific to Don Quixote)\n",
    "\ttext = '\\n'.join(\n",
    "\t\tline \n",
    "\t\tfor line in text.split('\\n')\n",
    "\t\tif '.jpg' not in line\n",
    "\t)\n",
    "\n",
    "\t# Normalize the string to decompose Unicode characters\n",
    "\ttext = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\t# Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "\ttext = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\t# remove newlines and tabs\n",
    "\ttext = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "\t# put spaces around allowed punctuation\n",
    "\tfor char in allowed_punctuation:\n",
    "\t\ttext = text.replace(char, f' {char} ')\n",
    "\n",
    "\n",
    "\t# remove leading and trailing spaces\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# remove multiple spaces\n",
    "\twhile '  ' in text:\n",
    "\t\ttext = text.replace('  ', ' ')\n",
    "\n",
    "\n",
    "\t# remove all characters except (alphanumeric, allowed_punctuation, ' ')\n",
    "\ttext = ''.join(\n",
    "\t\t(\n",
    "\t\t\tchar \n",
    "\t\t\tif (\n",
    "\t\t\t\tchar.isalnum() \n",
    "\t\t\t\tor char in allowed_punctuation \n",
    "\t\t\t\tor char == ' '\n",
    "\t\t\t)\n",
    "\t\t\telse ' '\n",
    "\t\t)\n",
    "\t\tfor char in text \n",
    "\t)\n",
    "\n",
    "\t# convert to lowercase\n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = text.strip()\n",
    "\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6713ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "\ttext: str,\n",
    "\tprocess: bool = False,\n",
    ") -> list[str]:\n",
    "\tif process:\n",
    "\t\ttext = process_text(text)\n",
    "\treturn text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c274f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])\n",
    "DATA: str = \" \".join(process_text(x) for x in DATA_RAW)\n",
    "DATA_TOKENIZED: list[str] = tokenize(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7248308",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 10\n",
    "d_vocab = 10\n",
    "d_hidden = 10\n",
    "n_context = 10\n",
    "\n",
    "x = torch.rand(1, 10)\n",
    "\n",
    "conf = Config(d_model, d_vocab, d_hidden, n_context)\n",
    "mlp = MLP(conf)\n",
    "attention = Attention(conf)\n",
    "Aoutput = attention(x)\n",
    "output = mlp(x)\n",
    "print(output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
