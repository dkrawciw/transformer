{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e0c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "import unicodedata\n",
    "\n",
    "from jaxtyping import Int, Float\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55346da0",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ce564",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int\n",
    "    d_vocab: int\n",
    "    d_hidden: int\n",
    "    n_context: int\n",
    "    n_layers: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Embedding(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "    \n",
    "#     def forward(self):\n",
    "#         pass\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        # self.W_qk = nn.Linear(config.d_model, config.d_vocab)\n",
    "        self.bilinear = nn.Bilinear(config.d_model, config.d_model, config.n_context, bias=False)\n",
    "        self.M = torch.triu(torch.ones((config.n_context, config.n_context)), diagonal=1)\n",
    "        self.M = self.M.masked_fill(self.M.bool(), -torch.inf)\n",
    "        self.second_matmult = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        xwx = self.bilinear(x, x) # d_m x d_m\n",
    "        x_masked = xwx+ self.M \n",
    "        x_softmaxed = self.softmax(x_masked)\n",
    "        x_fin = x_softmaxed@x\n",
    "        #multiply softmaxed by x\n",
    "        #multiply that by wov\n",
    "        x_fin = self.second_matmult(x_fin)\n",
    "        return x_fin\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear_up = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear_down = nn.Linear(config.d_hidden, config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_up(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_down(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.MLP = MLP(config=self.config)\n",
    "        self.Attention = Attention(config=self.config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.Attention(x) + self.MLP(x)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config:Config):\n",
    "        self.transformerBlock = nn.ModuleList([TransformerBlock(config) for i in range(config.n_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, l in enumerate(self.transformerBlock):\n",
    "            x = self.transformerBlock[i](x)\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7ee2d",
   "metadata": {},
   "source": [
    "$n_c$: Context window length\n",
    "\n",
    "$d_m$: Model Dimension\n",
    "\n",
    "$d_v$: Vocab Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "937b0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"The quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19deb5",
   "metadata": {},
   "source": [
    "# Tokenization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "549c941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_gutenberg_book(\n",
    "\tid: int | None = 84,\n",
    "\tdata_temp: Path | str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp: Path = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse: requests.Response = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path | str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c1b9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(\n",
    "\ttext: str,\n",
    "\tallowed_punctuation: str = \"-.,;:!?()\\\"\\\\\" + \"\".join(str(x) for x in range(10)),\n",
    "\tpunctuation_convert: dict[str, str] = {'â€”': '-'},\n",
    ") -> str:\n",
    "\t\n",
    "\t# replace some special characters which unicode won't normalize properly\n",
    "\tfor char, replacement in punctuation_convert.items():\n",
    "\t\ttext = text.replace(char, replacement)\n",
    "\n",
    "\t# if a line has \".jpg\" in it, remove that line (this is specific to Don Quixote)\n",
    "\ttext = '\\n'.join(\n",
    "\t\tline \n",
    "\t\tfor line in text.split('\\n')\n",
    "\t\tif '.jpg' not in line\n",
    "\t)\n",
    "\n",
    "\t# Normalize the string to decompose Unicode characters\n",
    "\ttext = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "\t# Encode to ASCII bytes, then decode back to string, ignoring errors\n",
    "\ttext = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\t# remove newlines and tabs\n",
    "\ttext = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "\n",
    "\n",
    "\t# put spaces around allowed punctuation\n",
    "\tfor char in allowed_punctuation:\n",
    "\t\ttext = text.replace(char, f' {char} ')\n",
    "\n",
    "\n",
    "\t# remove leading and trailing spaces\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# remove multiple spaces\n",
    "\twhile '  ' in text:\n",
    "\t\ttext = text.replace('  ', ' ')\n",
    "\n",
    "\n",
    "\t# remove all characters except (alphanumeric, allowed_punctuation, ' ')\n",
    "\ttext = ''.join(\n",
    "\t\t(\n",
    "\t\t\tchar \n",
    "\t\t\tif (\n",
    "\t\t\t\tchar.isalnum() \n",
    "\t\t\t\tor char in allowed_punctuation \n",
    "\t\t\t\tor char == ' '\n",
    "\t\t\t)\n",
    "\t\t\telse ' '\n",
    "\t\t)\n",
    "\t\tfor char in text \n",
    "\t)\n",
    "\n",
    "\t# convert to lowercase\n",
    "\ttext = text.lower()\n",
    "\n",
    "\ttext = text.strip()\n",
    "\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6713ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "\ttext: str,\n",
    "\tprocess: bool = False,\n",
    ") -> list[str]:\n",
    "\tif process:\n",
    "\t\ttext = process_text(text)\n",
    "\treturn text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c274f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 84...\n",
      "\t419422 characters read\n",
      "Getting book 15...\n",
      "\t1238469 characters read\n",
      "Getting book 18...\n",
      "\t1172825 characters read\n",
      "Getting book 82...\n",
      "\t1103796 characters read\n",
      "Getting book 996...\n",
      "\t2299352 characters read\n",
      "Getting book 2600...\n",
      "\t3208337 characters read\n"
     ]
    }
   ],
   "source": [
    "# Getting books from Plato and Aristotle\n",
    "DATA_RAW: list[str] = get_many_books([6762, 1497, 8438, 1600, 1656])\n",
    "DATA: str = \" \".join(process_text(x) for x in DATA_RAW)\n",
    "DATA_TOKENIZED: list[str] = tokenize(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "317fb7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ENCODED = array([4675,   14,    0, ...,  320,    4,  422], shape=(2176323,))\n",
      "2176323\n"
     ]
    }
   ],
   "source": [
    "# sorted by frequency\n",
    "VOCAB_FREQ: Counter[str] = Counter(DATA_TOKENIZED)\n",
    "VOCAB_ARR: list[str] = [word for word, _ in VOCAB_FREQ.most_common()]\n",
    "VOCAB_DICT: dict[str, int] = {word: i for i, word in enumerate(VOCAB_ARR)}\n",
    "\n",
    "def encode(\n",
    "\ttext: str | list[str],\n",
    ") -> Int[np.ndarray, \" n_tokens\"]:\n",
    "\tif isinstance(text, str):\n",
    "\t\ttext = tokenize(text)\n",
    "\treturn np.array([VOCAB_DICT[word] for word in text])\n",
    "\n",
    "def decode(\n",
    "\tencoded_text: Int[np.ndarray, \" n_tokens\"] | list[int],\n",
    ") -> str:\n",
    "\treturn ' '.join(VOCAB_ARR[i] for i in encoded_text)\n",
    "\n",
    "DATA_ENCODED: Int[np.ndarray, \" n_tokens\"] = encode(DATA)\n",
    "\n",
    "print(f\"{DATA_ENCODED = }\")\n",
    "print(len(DATA_ENCODED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38824d3",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7248308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "tensor([[ 0.2862,  0.0475, -0.1215,  0.0995,  0.0951, -0.3010,  0.0202, -0.0497,\n",
      "          0.4585,  0.0267],\n",
      "        [ 0.2735, -0.2232, -0.0666,  0.5076,  0.3087, -0.4875, -0.1118, -0.4132,\n",
      "          0.1509,  0.2209],\n",
      "        [ 0.1298,  0.1163, -0.5592,  0.2264,  0.0268, -0.8176,  0.3348,  0.2700,\n",
      "          0.6503,  0.1981],\n",
      "        [-0.0304,  0.0858,  0.1060,  0.1452,  0.3341, -0.2087, -0.0563, -0.0702,\n",
      "          0.5080, -0.3199],\n",
      "        [ 0.2071, -0.5021,  0.2310,  0.2624,  0.6465, -0.2821, -0.0022, -0.2098,\n",
      "         -0.1127, -0.0169]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sawyer\\Documents\\decodingGPT\\transformer\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "d_model = 10\n",
    "d_vocab = 10\n",
    "d_hidden = 10\n",
    "n_context = 5\n",
    "n_layers = 10\n",
    "\n",
    "x = torch.randn((n_context, d_model))\n",
    "\n",
    "conf = Config(d_model, d_vocab, d_hidden, n_context, n_layers)\n",
    "mlp = MLP(conf)\n",
    "attention = Attention(conf)\n",
    "Aoutput = attention(x)\n",
    "print(Aoutput.shape)\n",
    "\n",
    "output = mlp(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758f832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8475, -2.0726,  1.0514,  0.8324, -0.4577, -1.6299,  0.7028,  2.8054,\n",
       "         -0.8925, -1.0399],\n",
       "        [ 2.9761, -0.6997, -0.6032,  2.6684,  0.1813,  1.4837, -0.1221,  0.9587,\n",
       "          0.3077, -1.1505],\n",
       "        [ 2.3151,  1.8852, -0.4046,  1.1583, -1.5020, -0.6166, -0.8820,  1.2308,\n",
       "         -0.0742, -1.2055],\n",
       "        [ 1.3883, -0.1457,  0.3347,  2.0383,  0.6824,  1.7610, -1.7258,  0.1451,\n",
       "          0.2311, -2.7143],\n",
       "        [-0.5221,  0.6496, -2.3000,  0.4011,  0.5412, -1.5619, -1.4592, -0.0642,\n",
       "         -1.2541, -0.4855]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer Block test\n",
    "\n",
    "d_model = 10\n",
    "d_vocab = len(VOCAB_DICT)\n",
    "d_hidden = 10\n",
    "n_context = 5\n",
    "n_layers = 10\n",
    "\n",
    "config = Config(\n",
    "    d_model = d_model,\n",
    "    d_vocab = d_vocab,\n",
    "    d_hidden = d_hidden,\n",
    "    n_context = n_context,\n",
    "    n_layers = n_layers,\n",
    ")\n",
    "\n",
    "x = torch.randn((n_context, d_model))\n",
    "conf = Config(d_model, d_vocab, d_hidden, n_context, n_layers)\n",
    "\n",
    "tb = TransformerBlock(config)\n",
    "\n",
    "output_x = tb(x)\n",
    "output_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ed903",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 10\n",
    "d_vocab = len(VOCAB_DICT)\n",
    "d_hidden = 10\n",
    "n_context = 10\n",
    "n_layers = 10\n",
    "\n",
    "conf = Config(d_model, d_vocab, d_hidden, n_context, n_layers)\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=conf.d_vocab, embedding_dim=conf.d_model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
